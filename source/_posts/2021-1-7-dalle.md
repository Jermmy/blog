---
title: Dalle
date: 2021-01-07 22:43:45
tags: [深度学习]
categories: 深度学习
mathjax: true
---

今天在公司摸鱼看了 OpenAI 刚刚最新发布的 DALL•E，这是一个根据自然语言来生成图片的模型，说白就是你说一句话，它就可以生成一副对应的图片。比如说，你让它生成一张带蓝帽子、红手套、绿衬衣和黄裤子的企鹅宝宝，它果然就办到了，而且这些图片都是网上没有的，纯机器生成：

<center>
  <img src="/images/2021-1-7/dalle.jpg" width="500px">
</center>

「当然 OpenAI 还没有公布代码和论文，不排除有 cheery-pick 的可能，但我选择相信 OpenAI 的节操」

<!--more-->

别看仍然有些不和谐的地方，但 GAN 刚出来的时候也是这样，这些问题都是花时间能解决的，关键是 OpenAI 证明了 transformer 可以。

没想到离开学术界一年多，江湖已经变了天。想当初刚毕业的时候，transformer 刚刚成为 NLP 事实上的标准，结果工作一年半后已经攻下了 CV 界的半壁江山。从识别、检测到分割，现在连图像生成这种高难度回归任务也被攻克了「虽然生成的图片还不如 GAN，但已经有鼻子有眼的，质变已经达成」。

话说当年读研的时候，我还畅想过一个这样的项目：用一个爬虫程序从网上爬取图片和文字，然后用一个模型不断学习，监督信号先根据文字进行自监督，然后跨模态到图像领域进行学习，等一年后再回过头来看看这个模型进化到何种程度了。之所以会有这种想法，是因为人类的语言本身自带高层语义，因此通过自监督的方式完全可以学出点花样「word2vec 就是类似的原理」，基于此可以进一步监督对图像信号的学习「只要爬取内容的时候能找出文字和图片的关联」。这里面有一个很大的鸿沟是自然语言的信号怎么和图像信号沟通，说的学术一点，就是两种模态的特征之间如何 align。

现在看来这道鸿沟已经被 transformer 跨过去了。其实文字信号和图片信号的编码本身都多少需要依赖一定范围内的信号进行处理，文字信号的这种依赖会更严重，范围也更大，因此 RNN 的结构会有优势，而图片信号则是局部到全局的逐步依赖关系「比如图像识别里面，从细节逐步扩大到整个物体就可以判断出物体类别是啥」，这时 CNN 的结构优势就更明显。不过，transformer 天生就擅长处理各种依赖关系，换句话说，CNN 和 RNN 能做的事，它其实都能做，那统一只是时间问题。

可以预见，以后去噪去模糊去雨去雾人脸修复超分都会变成 transformer 的天下，再来点 NLP 高阶语义加成，我仿佛看到一堆论文要喷薄而出，被 CNN 捶打的日子不多了。

Jeff Dean 说过，2021年是多模态融合的一年。后面准备开干 transformer，跳出舒适区，早日布局「主要也是 CNN 已经快玩到尽头了」。