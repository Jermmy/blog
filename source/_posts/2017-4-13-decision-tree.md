---
title: 决策树生成算法
date: 2017-04-13 12:10:03
tags: [Machine Learning]
categories: Machine Learning
mathjax: true
---

关于决策树，想必大部分人都已经耳熟能详了，这是一种用来预测行为的树状分叉结构。本文主要想总结一下最常用的决策树生成算法：**[ID3](https://en.wikipedia.org/wiki/ID3_algorithm)**，**[C4.5](https://en.wikipedia.org/wiki/C4.5_algorithm)** 以及 **[CART](https://en.wikipedia.org/wiki/Predictive_analytics#Classification_and_regression_trees_.28CART.29)**。

### 构造的原则

熟悉决策树的你一定记得，决策树每个非叶子结点对应的其实是一个属性。比方说，想判断一个男生是不是 gay，我们首先需要判断他的性别是不是男的，是的话继续判断他的性取向，之后继续判断他的其他行为......这里的「性别」，「性取向」就是属性，而决策树的生成其实是依次挑选这些属性组成自己的节点，到最终可以明确得出结论的时候（也就是叶子结点），整棵树便生成了。所以，我们的目标就是按照某种方法依次挑选出这些属性。

我们挑选的原则是：每次选出这个属性后，可以最大限度地减小分类的可能性。回到 gay 这个问题，如果摆在我们眼前的属性有：「性取向」，「是否喜欢日漫」，「是否长发披肩」，那么，选择「性取向」这个属性，对我们之后的判断，帮助无疑是最大的。因为得知「性取向」之后，基本也就得到结论了。所以，对这个例子而言，「性取向」是我们优先挑选的属性。

那么，我们如何衡量这种帮助的大小呢？请往下看👇。

<!--more-->


### ID3 算法

ID3 算法归根到底就是提出一种合理的选择属性的方法。

（注意，决策树是一种知识学习算法，只有从众多样本中才能得出哪个属性最好，所以，构造决策树的前提是有大量的样本可供学习）

下面，为了方便讲解，我们需要引入信息学中**「熵」**的概念🙈。
#### 熵（entropy）

第一次接触熵的概念是在学高中化学的时候，课本告诉我们：一堆整齐有序的分子，最终都会演变成一个混乱复杂的群体，也就是，这个系统的熵值会逐渐变大。因此，简单整齐的系统，熵越小，越混乱的系统，熵越大。接下来，让我们回顾一下分子的布朗运动......

开个玩笑啦🤗。

同化学里的熵一样，信息学的熵也有类似的作用。在信息学中，如果熵越大，证明掌握的信息越少，事情越不确定。看到这里，你有没有觉得，熵的定义和我们前面提出的挑选属性的原则有点类似。是的，ID3 的精髓也就是在这，它通过计算属性的熵，来得出一个属性对事情的确定性能产生多大的影响，从而选出最好的属性。

那么熵该如何度量呢？

著名的信息论创始人「香农」提出一个度量熵的方法：假设有一堆样本 D，那么 D 的熵可以这样计算：
$$
H(D)=-\sum_{i=1}^{m}{p_ilog_2(p_i)}
$$
其中，$p_i$ 表示第 i 个类别在整个样本中出现的概率。
举个例子吧。假设我们投掷 10 枚硬币，其中，5 枚正面朝上，5 枚正面朝下，那么我们总共得到 10 个样本，这堆样本的熵为：

$H(D)=-(\frac{5}{10}log_2{\frac{5}{10}} + \frac{5}{10}log_2{\frac{5}{10}})=1$

反之，如果只有 1 枚硬币正面朝上，9 枚硬币正面朝下，那么熵为：

$H(D)=-(\frac{1}{10}log_2{\frac{1}{10}} + \frac{9}{10}log_2{\frac{9}{10}})=0.469$

如果全部硬币正面朝上，你应该可以算出来，熵为 0。
举这个例子是想说明：当熵的值越大的时候，事情会更加难以确定，如果你知道 10 次实验中，正面朝上的为 5 次，朝下的也为 5 次，那么下一次哪一面朝上，你是不是很难确定。相反，如果熵的值越小，事情就越明朗。当熵为 0，也就是 10 次都正面朝上的时候，下一次你会不会觉得正面朝上的概率会大很多（请忘掉你的传统思维，我没说这是一枚正常的硬币）。
#### 选择属性
好了，有了熵的概念以及度量方法，下面我们可以正式地走一遍 ID3 的流程了。
同样的，假设我们有一堆数据 D，我们先计算出这堆样本的熵$H(D)$，接下来，我们要对每个属性对信息的价值进行评估。假设我们挑选出 A 属性，那么，根据 A 属性的类别，我们可以把这堆样本分成几个子样本，每个子样本都对应 A 属性中的一类。我们继续按照熵的定义计算这几个子样本的熵，由于它们的熵是挑选出 A 后剩下的，因此我们定义为：
$$
Remainder(A)=\sum_{j=1}^{v}\frac{|D_j|}{|D|}H(D_j)
$$
这个公式其实和之前的是一个道理，我们通过 A 将 D 分成几个子集 $D_j$，这个时候，我们仍然需要计算这堆样本的熵，因此，先分别计算出每个 $D_j$ 的熵，然后乘以这个 $D_j$ 样本占所有数据集的比重，最后将全部子集的熵加起来即可。前面说了，这个熵是挑选 A 后剩下的，那么很自然的，我们想知道 A 到底帮助消减了多少熵，于是，我们定义最后一个公式，即**信息增益**：
$$
Gain(A)=H(D)-Remainder(A)
$$
之前对熵的说明告诉我们，熵越大，信息越少，反之，信息越多。$Gain(A)$其实就是 A 对信息的确定作用，它是我们选出 A 属性后，信息的混乱程度减少的量。
好了，到这里，ID3 的关键部分已经讲完了。其实，每次挑选属性的时候，我们都是计算出所有属性的**信息增益**，选择最大的作为分裂的属性，将样本分成几个子集，然后，对每个子集，继续选出最好的属性，继续分裂，直到所有样本都属于同一类为止（都是 gay 或者都是正面朝上）
#### 举个例子
下面用的这个例子摘自文末的参考博客[算法杂货铺——分类算法之决策树(Decision tree)](http://www.cnblogs.com/leoo2sk/archive/2010/09/19/decision-tree.html)。
假设我们有以下这堆 SNS 社区的资料，我们想确定一个账号是否是真实。
![example](/images/2017-4-13/eg.png)
其中，s 、m 和 l 分别表示小、中和大。
我们先计算出这堆样本的熵：

$H(D)=-(0.7\*log_2{0.7}+0.3\*log_2{0.3}) = 0.879$

然后，我们计算每个属性的信息增益：

$
\begin{align}
Remainder(L)=&0.3\*(-\frac{0}{3}log\_2{0}{3}-\frac{3}{3}log_2{\frac{3}{3}})\\\\&+0.4\*(-\frac{1}{4}log\_2\frac{1}{4}-\frac{3}{4}log\_2\frac{3}{4}\\\\&+0.3\*(-\frac{1}{3}log\_2\frac{1}{3}))=0.603 
\end{align}$

$Gain(L)=0.879-0.603=0.276$

同样的道理：

$Gain(F)=0.553$

$Gain(H)=0.033$

经过比较，我们发现 F 的增益最高，于是选出 F 作为节点，构造出如下决策树：
![tree1](/images/2017-4-13/tree1.png)
注意，F 属性有三个类别，对应三个分支，其中，l 和 m 两个分支的数据都是同一类（账号真实性要么都是 no 要么都是 yes），因此这两个分支没法再分了，而 s 属性的分支，剩下一个四个样本的子集，我们之后的任务，是对这个子集继续分割，直到没法再分为止。
接下来要考虑 L 和 H 属性，同样的道理，我们继续计算增益，只不过这一次我们是在这个子集上计算。

$H(D)=-(\frac{3}{4}\*log\_2{\frac{3}{4}}+\frac{1}{4}\*log\_2{\frac{1}{4}})=0.811$

$Remainder(L)=\frac{1}{2}\*(0)+\frac{1}{2}(-\frac{1}{2}log\_2{1}{2}-\frac{1}{2}log\_2{1}{2})=0.5$

$Remainder(H)=\frac{3}{4}\*[-\frac{2}{3}log\_2(\frac{2}{3})-\frac{1}{3}log\_2(\frac{1}{3})]+\frac{1}{4}\*0=0.689$

$Gain(L)=0.811-0.5=0.311$

$Gain(H)=0.811-0.689=0.122$

这一次，我们选择 L 属性进行分裂：
![tree2](/images/2017-4-13/tree2.png)
剩下的只有 H 属性，因此最后加上 H 节点。由于剩下的样本中只有 H=no 的数据，因此 yes 节点的数据没法判断（这种情况在数据量很大的时候一般不会遇到，因为数据量越大，涵盖的情况会更多），而剩下的两个样本存在 yes 和 no 两种情况，因此 no 节点往下也只能随机选择一种类别进行判断（这种情况一般是根据进行「多数表决」，即选择出现次数最多的类别作为最终类别，在数据量很大的情况下，出现次数一样多的情况几乎不会发生）。

#### 属性为连续值的情况

上面给出的例子中，样本的特征都是离散值（e.g.  s，m，l），而 ID3 算法确实也只对离散值起作用。如果遇到特征为连续值的情况，一般需要先将其离散化，例如：可以选定几个阈值$a\_1$，$a\_2$，$a\_3$（$a\_1<a\_2<a\_3$），根据这些阈值，将样本特征分为四类：$f < a\_1$，$a\_1 < f < a\_2$，$a\_2 < f < a\_3$，$f > a\_3$。然后，便可以按照一般的思路构建决策树了。

### C4.5算法
C4.5 算法主要对 ID3 进行了改进，用「增益率」来衡量属性的信息增益效率。算法中定义了「分裂信息」：

$SplitInfo(A)=-\sum_{j=1}^{v}{\frac{|D\_j|}{|D|}log\_2{\frac{|D\_j|}{|D|}}}$

然后，通过该信息，定义增益率公式为：

$GainRatio(A)=\frac{Gain(A)}{SplitInfo(A)}$。

C4.5选择具有最大「增益率」的属性作为分裂属性，而其余步骤，和 ID3 完全一致。

### CART

CART 指的是分类回归树（Classification And Regression Tree）。顾名思义，这是一棵既可以用于分类，也可以用于回归的树。不同于上面的两种树，CART 每一个非叶子节点只有有两个分支，所以 CART 是一棵二叉树。下面我们按照分类和回归两个用途分别介绍 CART 的构建。

#### 分类树的生成

CART 在选择分裂节点的时候，用「基尼指数（Gini）」来挑选最合适的特征进行分裂。所谓「基尼指数」，其实和 ID3 中熵的作用类似。假设我们有一个数据集 D，其中包含 N 个类别，那么「基尼指数」为：
$$
Gini(D) = 1 - sum_{j=1}^{N}{P_j^2}
$$
其中，$P_J$表示每个特征出现的概率。
同熵一样，「基尼系数」的值越小，样本越纯，分类越容易。
根据 Gini ，我们要选择一个最合适的特征作为 CART 的节点，方法同 ID3 一致，假设我们选择 A 作为分裂的特征，那么根据 A 特征我们可以将样本分为几个子集（例如前面例子中的日志密度按照 s、l、m 可以将样本分为三份），我们计算这个分裂后的样本的「基尼系数」：
$$
Gini(D, A) = \sum_{j}^{k}\frac{|D_j|}{|D|}
$$
其中，k 表示分裂的子集数目。
这个计算套路和 ID3 的$Remainder(A)$几乎一模一样。然后，选择$Gini(D, A)$最小的特征 A 作为分裂的特征即可。其实 ID3 也可以根据 $Remainder(A)$ 的最小值选择 A，跟 $Gain(A)$ 选择最大值的做法是一个意思，只不过习惯上人们更喜欢用信息增益 $Gain(A)$ 罢了。
这里还需要注意一点，在 ID3 中，已经选择过的特征是没法在之后的节点分裂中被选上的，即每个特征只能被挑选一次。但 CART 没有这种限制，每次都是将所有特征放在一起，通过「基尼系数」选出最好的，哪怕这个特征已经在之前被挑选过了。有学者认为，ID3 这种只挑选一次的做法容易导致 overfitting 问题，所以相信 CART 的这种做法能使模型的泛化能力更强。
CART 按照这样的方式，不断挑选特征分裂子数，直到剩下的子样本都属于同一类别，或者特征没法再分为止，这个停止条件也和 ID3 一致，这里就不再举例说明，

#### 回归树的生成





### 延伸

我们前面讨论的这些例子中，属性都是离散变量，而且构建出来的决策树也只能用于分类。而事实上，决策树对于连续的属性同样适用，而且在回归问题上也存在大量应用。关于这一点，下次继续补充。



### 参考

+ [算法杂货铺——分类算法之决策树(Decision tree)](http://www.cnblogs.com/leoo2sk/archive/2010/09/19/decision-tree.html)
+ [ID3 algorithm](https://en.wikipedia.org/wiki/ID3_algorithm)
+ [C4.5 algorithm](https://en.wikipedia.org/wiki/C4.5_algorithm)
+ [决策树（主要针对CART）的生成与剪枝](http://www.cnblogs.com/lvlvlvlvlv/p/5856986.html)
+ [CART算法学习及实现](http://www.cnblogs.com/happyblog/archive/2011/09/30/2196901.html)