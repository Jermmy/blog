---
title: 决策树生成算法
date: 2017-04-13 12:10:03
tags: [Machine Learning]
categories: Machine Learning
mathjax: true
---

关于决策树，想必大部分人都已经耳熟能详了，这是一种用来预测行为的树状分叉结构。本文主要想总结一下最常用的决策树生成算法：**[ID3](https://en.wikipedia.org/wiki/ID3_algorithm)**，并稍微引申一下它的升级版**[C4.5](https://en.wikipedia.org/wiki/C4.5_algorithm)**。

### 构造的原则

熟悉决策树的你一定记得，决策树每个非叶子结点对应的其实是一个属性。比方说，想判断一个男生是不是 gay，我们首先需要判断他的性别是不是男的，是的话继续判断他的性取向，之后继续判断他的其他行为......这里的「性别」，「性取向」就是属性，而决策树的生成其实是依次挑选这些属性组成自己的节点，到最终可以明确得出结论的时候（也就是叶子结点），整棵树便生成了。所以，我们的目标就是按照某种方法依次挑选出这些属性。

我们挑选的原则是：每次选出这个属性后，可以最大限度地减小分类的可能性。回到 gay 这个问题，如果摆在我们眼前的属性有：「性取向」，「是否喜欢日漫」，「是否长发披肩」，那么，选择「性取向」这个属性，对我们之后的判断，帮助无疑是最大的。因为得知「性取向」之后，基本也就得到结论了。所以，对这个例子而言，「性取向」是我们优先挑选的属性。

那么，我们如何衡量这种帮助的大小呢？请往下看👇。

### ID3 算法

ID3 算法归根到底就是提出一种合理的选择属性的方法。

（注意，决策树是一种知识学习算法，只有从众多样本中才能得出哪个属性最好，所以，构造决策树的前提是有大量的样本可供学习）

下面，为了方便讲解，我们需要引入信息学中**「熵」**的概念🙈。
#### 熵（entropy）

第一次接触熵的概念是在学高中化学的时候，课本告诉我们：一堆整齐有序的分子，最终都会演变成一个混乱复杂的群体，也就是，这个系统的熵值会逐渐变大。因此，简单整齐的系统，熵越小，越混乱的系统，熵越大。接下来，让我们回顾一下分子的布朗运动......

开个玩笑啦🤗。

同化学里的熵一样，信息学的熵也有类似的作用。在信息学中，如果熵越大，证明掌握的信息越少，事情越不确定。看到这里，你有没有觉得，熵的定义和我们前面提出的挑选属性的原则有点类似。是的，ID3 的精髓也就是在这，它通过计算属性的熵，来得出一个属性对事情的确定性能产生多大的影响，从而选出最好的属性。

那么熵该如何度量呢？

著名的信息论创始人「香农」提出一个度量熵的方法：假设有一堆样本 D，那么 D 的熵可以这样计算：
$$
H(D)=-\sum_{i=1}^{m}{p_ilog_2(p_i)}
$$
其中，$p_i$ 表示第 i 个类别在整个样本中出现的概率。
举个例子吧。假设我们投掷 10 枚硬币，其中，5 枚正面朝上，5 枚正面朝下，那么我们总共得到 10 个样本，这堆样本的熵为：
$H(D)=-(\frac{5}{10}log_2{\frac{5}{10}} + \frac{5}{10}log_2{\frac{5}{10}})=1$
反之，如果只有 1 枚硬币正面朝上，9 枚硬币正面朝下，那么熵为：
$H(D)=-(\frac{1}{10}log_2{\frac{1}{10}} + \frac{9}{10}log_2{\frac{9}{10}})=0.469$
如果全部硬币正面朝上，你应该可以算出来，熵为 0。
举这个例子是想说明：当熵的值越大的时候，事情会更加难以确定，如果你知道 10 次实验中，正面朝上的为 5 次，朝下的也为 5 次，那么下一次哪一面朝上，你是不是很难确定。相反，如果熵的值越小，事情就越明朗。当熵为 0，也就是 10 次都正面朝上的时候，下一次你会不会觉得正面朝上的概率会大很多（请忘掉你的传统思维，我没说这是一枚正常的硬币）。
#### 选择属性
好了，有了熵的概念以及度量方法，下面我们可以正式地走一遍 ID3 的流程了。
同样的，假设我们有一堆数据 D，我们先计算出这堆样本的熵$H(D)$，接下来，我们要对每个属性对信息的价值进行评估。假设我们挑选出 A 属性，那么，根据 A 属性的类别，我们可以把这堆样本分成几个子样本，每个子样本都对应 A 属性中的一类。我们继续按照熵的定义计算这几个子样本的熵，由于它们的熵是挑选出 A 后剩下的，因此我们定义为：
$$
Remainder(A)=\sum_{j=1}^{v}\frac{|D_j|}{|D|}H(D_j)
$$
这个公式其实和之前的是一个道理，我们通过 A 将 D 分成几个子集 $D_j$，这个时候，我们仍然需要计算这堆样本的熵，因此，先分别计算出每个 $D_j$ 的熵，然后乘以这个 $D_j$ 样本占所有数据集的比重，最后将全部子集的熵加起来即可。前面说了，这个熵是挑选 A 后剩下的，那么很自然的，我们想知道 A 到底帮助消减了多少熵，于是，我们定义最后一个公式，即**信息增益**：
$$
Gain(A)=H(D)-Remainder(A)
$$
之前对熵的说明告诉我们，熵越大，信息越少，反之，信息越多。$Gain(A)$其实就是 A 对信息的确定作用，它是我们选出 A 属性后，信息的混乱程度减少的量。
好了，到这里，ID3 的关键部分已经讲完了。其实，每次挑选属性的时候，我们都是计算出所有属性的**信息增益**，选择最大的作为分裂的属性，将样本分成几个子集，然后，对每个子集，继续选出最好的属性，继续分裂，直到所有样本都属于同一类为止（都是 gay 或者都是正面朝上）
#### 举个例子
下面用的这个例子摘自文末的参考博客[算法杂货铺——分类算法之决策树(Decision tree)](http://www.cnblogs.com/leoo2sk/archive/2010/09/19/decision-tree.html)。
假设我们有以下这堆 SNS 社区的资料，我们想确定一个账号是否是真实。
![example](/images/2017-4-13/eg.png)
其中，s、m和l分别表示小、中和大。
我们先计算出这堆样本的熵：
$H(D)=-(0.7*log_2(0.7)+0.3*log_2(0.3))=0.879$
然后，我们计算每个属性的信息增益：
$Remainder(L)=0.3*(-\frac{0}{3}log_2{0}{3}-\frac{3}{3}log_2{\frac{3}{3}})+0.4*(-\frac{1}{4}log_2\frac{1}{4}-\frac{3}{4}log_2\frac{3}{4}+0.3*(-\frac{1}{3}log_2\frac{1}{3}))=0.603$
$Gain(L)=0.879-0.603=0.276$
同样的道理：
$Gain(F)=0.553$
$Gain(H)=0.033$
经过比较，我们发现 F 的增益最高，于是选出 F 作为节点，构造出如下决策树：
![tree1](/images/2017-4-13/tree1.png)
注意，F 属性有三个类别，对应三个分支，其中，l 和 m 两个分支的数据都是同一类（账号真实性要么都是 no 要么都是 yes），因此这两个分支没法再分了，而 s 属性的分支，剩下一个四个样本的子集，我们之后的任务，是对这个子集继续分割，直到没法再分为止。
接下来要考虑 L 和 H 属性，同样的道理，我们继续计算增益，只不过这一次我们是在这个子集上计算。
$H(D)=-(\frac{3}{4}*log_2{\frac{3}{4}}+\frac{1}{4}*log_2{\frac{1}{4}})=0.811$
$Remainder(L)=\frac{1}{2}*(0)+\frac{1}{2}(-\frac{1}{2}log_2{1}{2}-\frac{1}{2}log_2{1}{2})=0.5$
$Remainder(H)=\frac{3}{4}*[-\frac{2}{3}log_2(\frac{2}{3})-\frac{1}{3}log_2(\frac{1}{3})]+\frac{1}{4}*0=0.689$
$Gain(L)=0.811-0.5=0.311$
$Gain(H)=0.811-0.689=0.122$
这一次，我们选择 L 属性进行分裂：
![tree2](/images/2017-4-13/tree2.png)
剩下的只有 H 属性，因此最后加上 H 节点。由于剩下的样本中只有 H=no 的数据，因此 yes 节点的数据没法判断（这种情况在数据量很大的时候一般不会遇到，因为数据量越大，涵盖的情况会更多），而剩下的两个样本存在 yes 和 no 两种情况，因此 no 节点往下也只能随机选择一种类别进行判断（这种情况一般是根据进行「多数表决」，即选择出现次数最多的类别作为最终类别，在数据量很大的情况下，出现次数一样多的情况几乎不会发生）。

### C4.5算法
C4.5 算法主要对 ID3 进行了改进，用「增益率」来衡量属性的信息增益效率。算法中定义了「分裂信息」：
$split_info_A(D)=-\sum_{j=1}^{v}{\frac{|D_j|}{|D|}log_2{\frac{|D_j|}{|D|}}}$
然后，通过该信息，定义增益率公式为：
$Gain_ratio(A)=\frac{Gain(A)}{split_info(A)}$。
C4.5选择具有最大「增益率」的属性作为分裂属性，而其余步骤，和 ID3 完全一致。



### 延伸

我们前面讨论的这些例子中，属性都是离散变量，而且构建出来的决策树也只能用于分类。而事实上，决策树对于连续的属性同样适用，而且在回归问题上也存在大量应用。关于这一点，下次继续补充。



### 参考

+ [算法杂货铺——分类算法之决策树(Decision tree)](http://www.cnblogs.com/leoo2sk/archive/2010/09/19/decision-tree.html)
+ [ID3 algorithm](https://en.wikipedia.org/wiki/ID3_algorithm)
+ [C4.5 algorithm](https://en.wikipedia.org/wiki/C4.5_algorithm)